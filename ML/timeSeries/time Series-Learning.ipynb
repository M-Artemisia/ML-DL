{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Lesson 1: Time series as supervised learning](http://machinelearningmastery.com/time-series-forecasting-supervised-learning/)\n\n\n# [Leson 2: Load Time series Data](http://machinelearningmastery.com/load-explore-time-series-data-python/)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Ref: https://machinelearningmastery.com/gentle-introduction-to-predictive-modeling/\n#A Series is a one-dimensional array with a time label for each row.\n#The series has a name, which is the column name of the data column.\n\n\n\nseries = pd.read_csv(\"/kaggle/input/femalebirthday/daily-total-female-births.csv\", header=0, index_col=0, parse_dates=True, squeeze=True)\nseries1 = pd.read_csv(\"/kaggle/input/femalebirthday/daily-total-female-births.csv\", header=0, index_col=0, parse_dates=[0], squeeze=True)\n'''header=0: We must specify the header information at row 0.\nparse_dates=[0]: We give the function a hint that data in the first column contains dates that need to be parsed. \nThis argument takes a list, so we provide it a list of one element, which is the index of the first column.\nindex_col=0: We hint that the first column contains the index information for the time series.\nsqueeze=True: We hint that we only have one data column and that we are interested in a Series and not a DataFrame.\n   '''\n\ndf = pd.read_csv(\"/kaggle/input/femalebirthday/daily-total-female-births.csv\")\n\nprint (\"type is: \", type(series))\nprint ('series format with parse date=True tail: \\n', series.tail())\nprint (\"size is: \", series.size)\nprint ('\\n\\n')\n\nprint (\"type is: \", type(series1))\nprint ('series format with parsedate=[0], tail: \\n', series1.tail())\nprint (\"size is: \", series1.size)\nprint ('\\n\\n')\n\nprint (\"type is: \", type(df))\nprint ('Data Frame format tail: \\n', df.tail(10))\nprint (\"size is: \", df.size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is often easier to perform manipulations of your time series data in a DataFrame rather than a Series object.\n\nIn those situations, you can easily convert your loaded Series to a DataFrame as follows:","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print('change series type to DataFrame Type: \\n')\ndf2 = pd.DataFrame(series1)\nprint (df2.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print ('Querying by time. index bassed: \\n')\nprint(series['1959-01'])\n\nprint('This type of index-based querying can help to prepare summary statistics and plots while exploring the dataset.\\n\\n')\n\nprint('series describe: \\n')\nprint(series.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Time Series\nPlotting time series data, especially univariate time series, is an important part of exploring your data.\n\nThis functionality is provided on the loaded Series by calling the **plot()** function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(series)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Lessont3: Time Series Data Visualization with Python](https://machinelearningmastery.com/time-series-data-visualization-with-python/)\n\n\nLine plots of observations over time are popular, but there is a suite of other plots that you can use to learn more about your problem.\n\nThe more you learn about your data, the more likely you are to develop a better forecasting model.\n\nIn this tutorial, you will discover 6 different types of plots that you can use to visualize time series data with Python.\n\nSpecifically, after completing this tutorial, you will know:\n\n* How to explore the temporal structure of time series with line plots, lag plots, and autocorrelation plots.\n* How to understand the distribution of observations using histograms and density plots.\n* How to tease out the change in distribution over intervals using box and whisker plots and heat map plots.\n* Discover how to prepare and visualize time series data and develop autoregressive forecasting models in my new book, with 28 step-by-step tutorials, and full python code.\n\n\nVisualization plays an important role in time series analysis and forecasting.\n\nPlots of the raw sample data can provide valuable diagnostics to identify temporal structures like trends, cycles, and seasonality that can influence the choice of model.\n\nA problem is that many novices in the field of time series forecasting stop with line plots.\n\nIn this tutorial, we will take a look at 6 different types of visualizations that you can use on your own time series data. They are:\n\n* Line Plots.\n* Histograms and Density Plots.\n* Box and Whisker Plots.\n* Heat Maps.\n* Lag Plots or Scatter Plots.\n* Autocorrelation Plots.\n\nThe focus is on univariate time series, but the techniques are just as applicable to multivariate time series, when you have more than one observation at each time step.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n#OR from matplotlib import pyplot as plt\n\ntemp_series = pd.read_csv(\"/kaggle/input/dailyminimumtemperatures/daily-min-temperatures.csv\", \n                          header=0, index_col=0, parse_dates=True, squeeze=True)\nprint (temp_series.head())\n\nplt.plot(temp_series)\n#OR temp_series.plot()\nplt.show()\n\nprint ('plt.plot(variable) does not work for DataFrame. it works perfectly by series')\n#plt.plot(df)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"temp_series.plot(style='k.') #OR style='k-'\n#This does not work:   plt.plot(temp_series, style='k.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be helpful **to compare line plots for the same interval**, such as from day-to-day, month-to-month, and year-to-year.\n\nThe Minimum Daily Temperatures dataset spans 10 years. We can group data by year and create a line plot for each year for direct comparison.\n\nThe groups are then enumerated and the observations for each year are stored as columns in a new DataFrame.\n\nFinally, a plot of this contrived DataFrame is created with each column visualized as a subplot with legends removed to cut back on the clutter.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from pandas import read_csv, DataFrame\nfrom pandas import Grouper\nfrom matplotlib import pyplot\nseries = read_csv('/kaggle/input/dailyminimumtemperatures/daily-min-temperatures.csv',\n                  header=0, index_col=0, parse_dates=True, squeeze=True)\nprint (series.head(),'\\n\\n\\n')\ngroups = series.groupby(Grouper(freq='A'))\nyears = DataFrame()\nprint ('groups:', groups,' its type: ', type(groups))\nfor name, group in groups:\n    #print (\"name is:\", name,'\\n\\n group is: ', group,'\\n\\n\\n')\n    #print (\"name.month is: \", name.month)\n    #print (\"name.year is: \", name.year, '\\n\\n\\n\\n')\n    years[name.year] = group.values\n     \nprint (years)\nprint ('draw plots for a Data frame in subplots!\\n')\nprint ('Running the example creates 10 line plots,',\n       'one for each year from 1981 at the top and 1990 at the bottom,',\n       ' where each line plot is 365 days in length.')\nyears.plot(subplots=True, legend=False)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([('bird', 'Falconiformes', 389.0),\n                      ('bird', 'Psittaciformes', 24.0),\n                       ('mammal', 'Carnivora', 80.2),\n                       ('mammal', 'Primates', np.nan),\n                       ('mammal', 'Carnivora', 58)],\n                      index=['falcon', 'parrot', 'lion', 'monkey', 'leopard'],\n                      columns=('class', 'order', 'max_speed'))\n\nprint (df, '\\n\\n\\n','groupby class')\ngrouped = df.groupby('class')\nfor x in grouped:\n    print (x)\n\nprint ('\\n\\n\\n\\n', 'groupby order axis=columns')\ngrouped = df.groupby('order')#, axis='columns')\nfor x in grouped:\n    print (x)\n    \nprint ('\\n\\n\\n\\n','groupby class, order')\ngrouped = df.groupby(['class', 'order'])\nfor x in grouped:\n    print (x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Time Series Histogram and Density Plots\nAnother important visualization is of the distribution of observations themselves.\n\nThis means a plot of the values without the temporal ordering.\n\nSome linear time series forecasting methods assume a well-behaved distribution of observations (i.e. a bell curve or normal distribution). This can be explicitly checked using tools like statistical hypothesis tests. But plots can provide a useful first check of the distribution of observations both on raw observations and after any type of data transform has been performed.\n\nThe example below creates a histogram plot of the observations in the Minimum Daily Temperatures dataset. A histogram groups values into bins, and the frequency or count of observations in each bin can provide insight into the underlying distribution of the observations.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"series.hist()\npyplot.show()\nprint ('Running the example shows a distribution that looks strongly Gaussian.',\n       ' The plotting function automatically selects the size of the bins based on',\n       'the spread of values in the data.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can get a better idea of the shape of the distribution of observations by using a **density plot**. \nThis is *like the histogram, except a function is used to fit the distribution of observations and* a nice, smooth line is used to summarize this distribution.\n\nBelow is an example of a density plot of the Minimum Daily Temperatures dataset.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"series.plot(kind='kde')\npyplot.show()\nprint ('Seeing a distribution like this may suggest later exploring statistical hypothesis',\n       'tests to formally check if the distribution is Gaussian and perhaps data preparation',\n       'techniques to reshape the distribution, like the Box-Cox transform.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Time Series Box and Whisker Plots by Interval\n\nHistograms and density plots provide insight into the distribution of all observations, but we may be interested in the distribution of values by time interval.\n\nAnother type of plot that is useful to summarize the distribution of observations is \n### **the box and whisker plot**. \n**This plot draws a box around the 25th and 75th percentiles of the data that captures the middle 50% of observations.** A line is drawn at the *50th percentile* (the median) and *whiskers are drawn above and below the box to summarize the general extents of the observations*. **Dots** are *drawn for outliers outside the whiskers or extents of the data*.\n\n\n**Box and whisker plots can be created and compared for each interval in a time series, such as years, months, or days.**\n\n\nBelow is an example of grouping the Minimum Daily Temperatures dataset by years, as was done above in the plot example. A box and whisker plot is then created for each year and lined up side-by-side for direct comparison.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"years.boxplot()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"''' \nThis example creates 12 box and whisker plots, one for each month of 1990, \nthe last year in the dataset.\n\nfirst, only observations from 1990 are extracted.\nThen, the observations are grouped by month, and each month is added to a new DataFrame\nas a column.\nFinally, a box and whisker plot is created for each month-column in the newly constructed DataFrame.\n'''\nfrom pandas import concat\n\none_year = series['1990']\ngroups = one_year.groupby(Grouper(freq='M'))\n#months = concat([DataFrame(x[1].values) for x in groups], axis=1)\n\nmonths = DataFrame()\nfor x in groups:\n    val = x[1].values\n    dfval = DataFrame(val)\n    months = concat([months,dfval], axis=1)\n\n#months = concat([DataFrame(x[1].values) for x in groups], axis=1)\n#months = DataFrame(months)\nmonths.columns = range(1,13)\n\nmonths.boxplot()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Time Series Heat Maps\n\nA matrix of numbers can be plotted as a surface, where the values in each cell of the matrix are assigned a unique color.\nThis is called a heatmap, as larger values can be drawn with warmer colors (yellows and reds) and smaller values can be drawn with cooler colors (blues and greens).\n\nLike the box and whisker plots, we can compare observations between intervals using a heat map.\n\nThe matshow() function from the matplotlib library is used as no heatmap support is provided directly in Pandas.\n\nIn the case of the Minimum Daily Temperatures, the observations can be arranged into a matrix of year-columns and day-rows, with minimum temperature in the cell for each day. A heat map of this matrix can then be plotted.\nFor convenience, the matrix is rotation (transposed) so that each row represents one year and each column one day. This provides a more intuitive, left-to-right layout of the data.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import Grouper\nfrom matplotlib import pyplot\n\nseries = read_csv('/kaggle/input/dailyminimumtemperatures/daily-min-temperatures.csv',\n                  header=0, index_col=0, parse_dates=True, squeeze=True)\ngroups = series.groupby(Grouper(freq='A'))\nyears = DataFrame()\nfor name, group in groups:\n\tyears[name.year] = group.values\nyears = years.T\npyplot.matshow(years, interpolation=None, aspect='auto')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"'''\nBelow is an example of a heat map comparing the months of the year in 1990. \nEach column represents one month, with rows representing the days of the month from 1 to 31.\n'''\n\none_year = series['1990']\n\ngrp = one_year.groupby(Grouper(freq='M'))\n\nmonths_one_year = concat ([DataFrame(x[1].values) for x in grp], axis =1)\nmonths_one_year = DataFrame(months_one_year)\n\nmonths_one_year.columns = range(1,13)\n\npyplot.matshow(months_one_year, interpolation=None, aspect='auto')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Time Series Lag Scatter Plots\n\nTime series modeling assumes a relationship between an observation and the previous observation.\n\nPrevious observations in a time series are called **lags**, with the observation at the previous time step called *lag1*, the observation at two time steps ago *lag2*, and so on.\n\n**A useful type of plot to explore the relationship between each observation and a lag of that observation is called the scatter plot.**\n\nPandas has a built-in function for exactly this called the lag plot. It plots the observation at time t on the x-axis and the lag1 observation (t-1) on the y-axis.\n\n* If the points cluster along a diagonal line from the bottom-left to the top-right of the plot, it suggests a positive correlation relationship.\n* If the points cluster along a diagonal line from the top-left to the bottom-right, it suggests a negative correlation relationship.\n* Either relationship is good as they can be modeled.\n\nMore points tighter in to the diagonal line suggests a stronger relationship and more spread from the line suggests a weaker relationship.\n\nA ball in the middle or a spread across the plot suggests a weak or no relationship.\n\nBelow is an example of a lag plot for the Minimum Daily Temperatures dataset.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a scatter plot\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom pandas.plotting import lag_plot\n\n#series = read_csv('daily-minimum-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\nlag_plot(series)\npyplot.show()\nprint ('The plot created from running the example shows a relatively strong positive correlation between observations and their lag1 values.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can repeat this process for an observation and any lag values. Perhaps with the observation at the same time last week, last month, or last year, or any other domain-specific knowledge we may wish to explore.\n\nFor example, we can create a scatter plot for the observation with each value in the previous seven days. Below is an example of this for the Minimum Daily Temperatures dataset.\n\nFirst, a new DataFrame is created with the lag values as new columns. The columns are named appropriately. Then a new subplot is created that plots each observation with a different lag value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom matplotlib import pyplot\nfrom pandas.plotting import scatter_matrix\n\n\n#series = read_csv('daily-minimum-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\nlags = 7\n\nvalues = DataFrame(series.values)\narrayVal = [values]\nfor i in range(1,(lags + 1)):\n    arrayVal.append(values.shift(i))\n\ndataframe = concat(arrayVal, axis=1)\n\n\ncolumns = ['t+1']\nfor i in range(1,(lags + 1)):\n    columns.append('t-' + str(i))\n\n    \ndataframe.columns = columns\n\n\npyplot.figure(1)\nfor i in range(1,(lags + 1)):\n    ax = pyplot.subplot(240 + i)\n    ax.set_title('t+1 vs t-' + str(i))\n    pyplot.scatter(x=dataframe['t+1'].values, y=dataframe['t-'+str(i)].values)\n\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Time Series Autocorrelation Plots\nWe can quantify the strength and type of relationship between observations and their lags.\n\nIn statistics, this is called **correlation**, and *when calculated against lag values in time series, it is called* **autocorrelation** (**self-correlation**).\n\nA correlation value calculated between two groups of numbers, such as observations and their lag1 values, results in a number between -1 and 1. *The sign of this number indicates a negative or positive correlation respectively*. A value close to zero suggests a **weak correlation**, whereas a value closer to *-1 or 1* indicates a **strong correlation**.\n\nCorrelation values, called **correlation coefficients**, *can be calculated for each observation and different lag values*. Once calculated, a plot can be created *to help better understand how this relationship changes over the lag*.\n\nThis type of plot is called an **autocorrelation plot** and Pandas provides this capability built in, called the autocorrelation_plot() function.\n\n\n\n\n\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom matplotlib import pyplot\nfrom pandas.plotting import autocorrelation_plot\n\n#series = read_csv('daily-minimum-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\nautocorrelation_plot(series)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resulting plot shows *lag along the x-axis* and the *correlation on the y-axis.* \n**Dotted** lines are provided that *indicate any correlation values above those lines are statistically significant *(meaningful).\n\nWe can see that for the Minimum Daily Temperatures dataset we see cycles of strong negative and positive correlation. This captures the relationship of an observation with past observations in the *same and opposite seasons or times of year*. **Sine waves like those seen in this example are a strong sign of seasonality in the dataset.**\n\n## Further Reading\nThis section provides some resources for further reading on plotting time series and on the Pandas and Matplotlib functions used in this tutorial.\n\n[Metric graphs 101: Timeseries graphs](https://www.datadoghq.com/blog/timeseries-metric-graphs-101/)\n\n[A Tour Through the Visualization Zoo](http://homes.cs.washington.edu/~jheer//files/zoo/)\n\n[Pandas Plotting](http://pandas.pydata.org/pandas-docs/version/0.19.0/visualization.html)\n\n[DataFrame Plot Function](http://pandas.pydata.org/pandas-docs/version/0.19.1/generated/pandas.DataFrame.plot.html)\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# [Lesson 04: Persistence Forecast Model](https://machinelearningmastery.com/persistence-time-series-forecasting-with-python/)\n\nA baseline in forecast performance provides a point of comparison.\n\nIt is a point of reference for all other modeling techniques on your problem. If a model achieves performance at or below the baseline, the technique should be fixed or abandoned.\n\nThe technique used to generate a forecast to calculate the baseline performance must be easy to implement and naive of problem-specific details.\n\nBefore you can establish a performance baseline on your forecast problem, you must develop a test harness. This is comprised of:\n\n* The **dataset** you intend to use to train and evaluate models.\n* The **resampling technique** you intend to use to estimate the performance of the technique (e.g. train/test split).\n* The **performance measure** you intend to use to evaluate forecasts (e.g. mean squared error).\n\nOnce prepared, you then need to select a naive technique that you can use to make a forecast and calculate the baseline performance.\n\nThe simplest forecast you can make is to use the\ncurrent observation (t) to predict the observation at the next time step (t+1). This is called\nthe naive forecast or the persistence forecast and may be the best possible model on some time\nseries forecast problems.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nfrom pandas import datetime\nfrom pandas import read_csv\nfrom matplotlib import pyplot as plt\n\ndef parser(x):\n    return datetime.strptime('190'+x, '%Y-%m')\n\nseries = read_csv('/kaggle/input/time-series-data/shampoo.csv',\n                 header=0, parse_dates=[0],index_col=0, squeeze=True)\nseries.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Persistence Algorithm\nA persistence model can be implemented easily in Python.\n\nWe will break this section down into 4 steps:\n* Transform the univariate dataset into a supervised learning problem.\n* Establish the train and test datasets for the test harness.\n* Define the persistence model.\n* Make a forecast and establish a baseline performance.\n* Review the complete example and plot the output.\n\nLet’s dive in.\n\n### Step 1: Define the Supervised Learning Problem\n\nThe first step is to load the dataset and create a lagged representation. That is, given the \nobservation at t-1, predict the observation at t+1.\n\nWe can see that the first row (index 0) will have to be discarded as there was no observation prior to the first observation to use to make the prediction.\n\nFrom a supervised learning perspective, the t-1 column is the input variable, or X, and the t+1 column is the output variable, or y.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x = series.shift(1)\ndf = concat([x, series], axis=1)\ndf.columns = ['t-1','t+1']\nprint (df.head(10))\n\n\nval = DataFrame(series.values)\ndf = concat([val.shift(1),val], axis=1)\ndf.columns = ['t-1','t+1']\nprint (df.head(10))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2: Train and Test Sets\nThe next step is to separate the dataset into train and test sets.\n\nWe will keep the first 66% of the observations for “training” and the remaining 34% for evaluation. During the split, we are careful to exclude the first row of data with the NaN value.\n\nNo training is required in this case; it’s just habit. Each of the train and test sets are then split into the input and output variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test sets\nX = df.values\ntrain_size = int(len(X) * 0.66)\ntrain, test = X[1:train_size], X[train_size:]\ntrain_X, train_y = train[:,0], train[:,1]\ntest_X, test_y = test[:,0], test[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Persistence Algorithm\nWe can define our persistence model as a function that returns the value provided as input.\n\nFor example, if the t-1 value of 266.0 was provided, then this is returned as the prediction, whereas the actual real or expected value happens to be 145.9 (taken from the first usable row in our lagged dataset).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# persistence model\ndef model_persistence(x):\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4: Make and Evaluate Forecast\nNow we can evaluate this model on the test dataset.\n\nWe do this using the walk-forward validation method.\n\nNo model training or retraining is required, so in essence, we step through the test dataset time step by time step and get predictions.\n\nOnce predictions are made for each time step in the training dataset, they are compared to the expected values and a Mean Squared Error (MSE) score is calculated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n#walk-forward validation\npredictions = list()\nfor x in test_X:\n    yhat = model_persistence(x)\n    predictions.append(yhat)\n    \ntest_score = mean_squared_error(test_y, predictions)\nprint('Test MSE: %.3f' % test_score)\n\nrms = sqrt(test_score)\nprint ('rms is : %.3f' % rms)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 5: Complete Example\nFinally, a plot is made to show the training dataset and the diverging predictions from the expected values from the test dataset.\n\nFrom the plot of the persistence model predictions, it is clear that the model is 1-step behind reality. There is a rising trend and month-to-month noise in the sales figures, which highlights the limitations of the persistence technique.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot predictions and expected results\npyplot.plot(train_y)\npyplot.plot([None for i in train_y] + [x for x in test_y])\npyplot.plot([None for i in train_y] + [x for x in predictions])\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The complete example:\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom matplotlib import pyplot\nfrom sklearn.metrics import mean_squared_error\n \ndef parser(x):\n     return datetime.strptime('190'+x, '%Y-%m')\n \nseries = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n# Create lagged dataset\nvalues = DataFrame(series.values)\ndataframe = concat([values.shift(1), values], axis=1)\ndataframe.columns = ['t-1', 't+1']\nprint(dataframe.head(5))\n \n# split into train and test sets\nX = dataframe.values\ntrain_size = int(len(X) * 0.66)\ntrain, test = X[1:train_size], X[train_size:]\ntrain_X, train_y = train[:,0], train[:,1]\ntest_X, test_y = test[:,0], test[:,1]\n \n# persistence model\ndef model_persistence(x):\n\treturn x\n \n# walk-forward validation\npredictions = list()\nfor x in test_X:\n\tyhat = model_persistence(x)\n\tpredictions.append(yhat)\ntest_score = mean_squared_error(test_y, predictions)\nprint('Test MSE: %.3f' % test_score)\n \n# plot predictions and expected results\npyplot.plot(train_y)\npyplot.plot([None for i in train_y] + [x for x in test_y])\npyplot.plot([None for i in train_y] + [x for x in predictions])\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Lesson 05: Autoregressive Forecast Model](http://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/)\n\n\nAutoregression means developing a *linear* model that *uses observations at previous time steps* to predict observations at future time step (auto means self in ancient Greek). Autoregression is a quick and powerful time series forecasting method. The **Statsmodels** Python library provides the autoregression model in the **AutoReg class**. In this lesson, you will develop an autoregressive forecast model for a standard time series dataset.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Autoregression\nA regression model, such as linear regression, models an output value based on a linear combination of input values. such as:\n\nyhat = b0 + b1*X1\n\nWhere yhat is the prediction, b0 and b1 are coefficients found by optimizing the model on training data, and X is an input value. \nThis technique can be used on time series where input variables are taken as observations at previous time steps, called lag variables.\n\nBecause the regression model uses data from the same input variable at previous time steps, it is referred to as an autoregression (regression of self).\n\n## Autocorrelation\n\nAn autoregression model makes an assumption that the observations at previous time steps are useful to predict the value at the next time step.\nThis relationship between variables is called **correlation**.\n\n* If both variables *change in the same direction* (e.g. go up together or down together), this is called a **positive correlation**. \n* If the variables move in *opposite directions* as values change (e.g. one goes up and one goes down), then this is called **negative correlation**.\n\n\nWe can use **statistical measures** to calculate the correlation between the output variable and values at previous time steps at various different lags. *The stronger the correlation between the output variable and a specific lagged variable, the **more weight** that autoregression model can put on that variable when modeling*. The correlation statistics can also help to choose which lag variables will be useful in a model and which will not.\n\n\nAgain, because the *correlation is calculated between the variable and itself at previous time steps*, it is called an **autocorrelation**. It is also called **serial correlation** because of the *sequenced structure of time series data*.\n\nInterestingly, if all lag variables show low or no correlation with the output variable, then it **suggests that the time series problem may not be predictable**. This can be very useful when getting started on a new dataset.\n\nIn this tutorial, we will investigate the autocorrelation of a univariate time series then develop an autoregression model and use it to make predictions.\n\n### Quick Check for Autocorrelation\nWe can plot the observation at the previous time step (t-1) with the observation at the next time step (t+1) as a scatter plot. \nThis could be done manually by first creating a lag version of the time series dataset and using a built-in scatter plot function in the Pandas library.\n\nBut there is an easier way.\nPandas provides a built-in plot to do exactly this, called the **lag_plot() function**.\n\n\n### Pearson correlation coefficient\n\nAnother quick check that we can do is to directly calculate the correlation between the observation and the lag variable.\nWe can use a statistical test like the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient). This produces a number to summarize how correlated two variables are between -1 (negatively correlated) and +1 (positively correlated) with small values close to zero indicating low correlation and high values above 0.5 or below -0.5 showing high correlation.\n\nCorrelation can be calculated easily using the **corr() function** on the *DataFrame* of the lagged dataset.\n\nThe example below creates a lagged version of the Minimum Daily Temperatures dataset and calculates a correlation matrix of each column with other columns, including itself.\n\n**Autocorrelation Plots** are a scaled-up version of this approach. Using them, We can plot the correlation coefficient for each lag variable.\n\nThis can very quickly *give an idea of which lag variables may be good candidates for use in a predictive model and how the relationship between the observation and its historic values changes over time*.\n\nWe could manually calculate the correlation values for each lag variable and plot the result. Thankfully, Pandas provides a built-in plot called the autocorrelation_plot() function.\nThe plot provides the *lag number along the x-axis and the correlation coefficient value between -1 and 1 on the y-axis*. The plot also includes **solid and dashed lines** that *indicate the 95% and 99% confidence interval for the correlation values*. Correlation values **above these lines** are more significant than those below the line, providing a threshold or cutoff for selecting more relevant lag values.\n\nThe statsmodels library also provides a version of the plot in the **plot_acf()** function as a line plot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv, DataFrame\nfrom matplotlib import pyplot as plt\n\ntmpr_series = read_csv('/kaggle/input/dailyminimumtemperatures/daily-min-temperatures.csv',\n                      header=0, parse_dates=[0], index_col=0, squeeze=True)\nprint (tmpr_series.head())\ntmpr_series.plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check Autocorrelatioln\nfrom pandas.plotting import scatter_matrix\nfrom pandas.plotting import lag_plot\nfrom pandas.plotting import autocorrelation_plot\n\nlag =1\n#lag_plot(tmpr_series)\nplt.figure(1)\nax = plt.subplot(121)\nax.set_title('lag_plot')\n#plt.scatter(x=dataframe['t+1'].values, y=dataframe['t-'+str(i)].values)\nlag_plot(tmpr_series,lag=lag, ax=ax)\n\nax = plt.subplot(122)\nax.set_title('autocorrelation_plot')\nautocorrelation_plot(tmpr_series, ax=ax)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running the example plots the temperature data (t) on the x-axis against the temperature on the previous day (t-1) on the y-axis.\n\nWe can see a large ball of observations along a diagonal line of the plot. It clearly shows a relationship or some correlation.\n\nThis process could be repeated for any other lagged observation, such as if we wanted to review the relationship with the last 7 days or with the same day last month or last year.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom matplotlib import pyplot\n\n#series = read_csv('daily-minimum-temperatures.csv', header=0, index_col=0)\nseries = tmpr_series\nvalues = DataFrame(series.values)\ndataframe = concat([values.shift(1), values], axis=1)\ndataframe.columns = ['t-1', 't+1']\nresult = dataframe.corr()\nprint(result)\nprint('It shows a strong positive correlation (0.77) between the observation and the lag=1 value.')\n\nautocorrelation_plot(tmpr_series)\nprint ('Running the example shows the swing in positive and negative correlation as the temperature values change across summer and winter seasons each previous year.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In this example, we limit the lag variables evaluated to 31 for readability.\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_acf\n\n#series = read_csv('daily-minimum-temperatures.csv', header=0, index_col=0)\nplot_acf(series, lags=31)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Persistence Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom matplotlib import pyplot\nfrom sklearn.metrics import mean_squared_error\n\n#series = read_csv('daily-minimum-temperatures.csv', header=0, index_col=0)\n\n# create lagged dataset\nvalues = DataFrame(series.values)\ndataframe = concat([values.shift(1), values], axis=1)\ndataframe.columns = ['t-1', 't+1']\n# split into train and test sets\nX = dataframe.values\ntrain, test = X[1:len(X)-7], X[len(X)-7:]\ntrain_X, train_y = train[:,0], train[:,1]\ntest_X, test_y = test[:,0], test[:,1]\n \n# persistence model\ndef model_persistence(x):\n    return x\n \n# walk-forward validation\npredictions = list()\nfor x in test_X:\n    yhat = model_persistence(x)\n    predictions.append(yhat)\ntest_score = mean_squared_error(test_y, predictions)\nprint('Test MSE: %.3f' % test_score)\n\n# plot predictions vs expected\npyplot.plot(test_y)\npyplot.plot(predictions, color='red')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AutoRegression Model\nAn autoregression model is a linear regression model that **uses lagged variables as input variables**.\n\nWe could calculate the linear regression model *manually* using the **LinearRegession class in scikit-learn** and *manually specify the lag input variables to use*.\n\nAlternately, the **statsmodels** library provides an autoregression model where *you must specify an appropriate lag value* and *trains a linear regression model*. It is provided in the **AutoReg class**.\n\nWe can use this model by first **creating the model AutoReg()** and then **calling fit()** to *train it on our dataset*. This returns an AutoRegResults object.\n\nOnce fit, we can use the model to make a prediction by calling the **predict()** function for a number of observations in the future. This creates 1 7-day forecast, which is different from the persistence example above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and evaluate a static autoregressive model\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# load dataset\n#series = read_csv('daily-minimum-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n\n# split dataset\nX = series.values\ntrain, test = X[1:len(X)-7],X[len(X)-7:] \n\n# train autoregression\nmodel = AutoReg(train, lags=29)\nmodel_fit = model.fit()\nprint('Coefficients: %s' % model_fit.params)\n\n# make predictions\npredictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=False)\nfor i in range(len(predictions)):\n    print('predicted=%f, expected=%f' % (predictions[i], test[i]))\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot results\npyplot.plot(test)\npyplot.plot(predictions, color='red')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The statsmodels API does not make it easy to update the model as new observations become available.\nOne way would be to re-train the AutoReg model each day as new observations become available, and that may be a valid approach, if not computationally expensive.\n\nAn alternative would be to use the learned coefficients and manually make predictions. This requires that the history of 29 prior observations be kept and that the coefficients be retrieved from the model and used in the regression equation to come up with new forecasts.\n\nThe coefficients are provided in an array with the intercept term followed by the coefficients for each lag variable starting at t-1 to t-n. We simply need to use them in the right order on the history of observations, as follows:\n\nyhat = b0 + b1*X1 + b2*X2 ... bn*Xn\n\nBelow is the complete example.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and evaluate an updated autoregressive model\nfrom pandas import read_csv\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# load dataset\n#series = read_csv('daily-minimum-temperatures.csv', \n#                  header=0, index_col=0, parse_dates=True, squeeze=True)\n\n# split dataset\nX = series.values\ntrain, test = X[1:len(X)-7], X[len(X)-7:]\n\n# train autoregression\nwindow = 29\nmodel = AutoReg(train, lags=29)\nmodel_fit = model.fit()\ncoef = model_fit.params\n\n# walk forward over time steps in test\nhistory = train[len(train)-window:]\nhistory = [history[i] for i in range(len(history))]\npredictions = list()\nfor t in range(len(test)):\n\tlength = len(history)\n\tlag = [history[i] for i in range(length-window,length)]\n\tyhat = coef[0]\n\tfor d in range(window):\n\t\tyhat += coef[d+1] * lag[window-d-1]\n\tobs = test[t]\n\tpredictions.append(yhat)\n\thistory.append(obs)\n\tprint('predicted=%f, expected=%f' % (yhat, obs))\nrmse = sqrt(mean_squared_error(test, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot\npyplot.plot(test)\npyplot.plot(predictions, color='red')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [Lesson 06: ARIMA Forecast Model](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)\n\nA popular and widely used statistical method for time series forecasting is the ARIMA model.\nARIMA is an acronym that stands for AutoRegressive Integrated Moving Average.  It is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration.\n\n* **AR**: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n* **I**: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n* **MA**: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\nA standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the specific ARIMA model being used.\n* **p**: The number of lag observations included in the model, also called the lag order.\n* **d**: The number of times that the raw observations are differenced, also called the degree of differencing.\n* **q**: The size of the moving average window, also called the order of moving average.\n\nA linear regression model is constructed including the specified number and type of terms, and the data is prepared by a degree of differencing in order to make it stationary, i.e. to remove trend and seasonal structures that negatively affect the regression model.\n\nA value of 0 can be used for a parameter, which indicates to not use that element of the model. This way, the ARIMA model can be configured to perform the function of an ARMA model, and even a simple AR, I, or MA model.\n\nThe stages in the process of applying ARIMA are:\n1. load dataset\n2. plot it to find out if the data has any trend or seasonality. if there is, use d to convert data o a stationary data (d)\n3. use autocorrelation to find out a proper number for lag (p)\n4. find a proper value for MA (q). currently we just use q=0\n5. use ARIMA\n   * Define model bu calling ARIMA(p,d,q) \n   * Train model using training data by calling fit()\n   * Use summary() on the fitted model to get info about the model \n        * This summarizes the coefficient values used as well as the skill of the fit on the on the in-sample observations.\n6. plot Residual Errors\n   * use residulas = model_fit.resid on the fitted model\n   * and describe it using residulas.describe()\n7. Predict by calling prediction() and specifing the index of the time or times to be predicted\n   * We can use the **predict()** on the ARIMAResults object or **forcast** to make predictions.\n   * We can split the training dataset into **train** and **test** sets, use the train set to *fit the model*, and generate a prediction for each element on the test set.\n   * A **rolling forecast** is required *given the dependence on observations in prior time steps* for differencing and the AR model. A crude way to perform this rolling forecast is \n      * to re-create the ARIMA model after each new observation is received. \n      * We manually keep track of all observations in a list called *history* that is seeded with the training data\n      * and to which new observations are appended each iteration.\n\nlets do it!\n\n#### some Explaination about prediction mfunctions\n**predict()** accepts the index of the time steps to make predictions as arguments. These indexes are relative to the start of the training dataset used to make predictions. If we used 100 observations in the training dataset to fit the model, then the index of the next time step for making a prediction would be specified to the prediction function as start=101, end=101. This would return an array with one element containing the prediction.\nWe also would prefer the forecasted values to be in the original scale, in case we performed any differencing (d>0 when configuring the model). This can be specified by setting the typ argument to the value ‘levels’: typ=’levels’.\n\nAlternately, we can avoid all of these specifications by using the **forecast()** function, which performs a one-step forecast using the model.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from pandas import read_csv, DataFrame \nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n#STEP1:\nshampoo_series = read_csv('/kaggle/input/time-series-data/shampoo.csv',\n                         header=0, parse_dates=True, index_col=0, squeeze=True)\nprint ('Step1: load dataset\\n',shampoo_series.head(7))\n\n#STEP2:\nprint('Step2: plot the dataset\\n')\nshampoo_series.plot()\nplt.show() #==> d>=1\n\n#STEP3:\nprint('Step3: correlation plot\\n')\nautocorrelation_plot(shampoo_series) #==> lag=q=5 !\n\n#STEP4: \n# ...  p=0\n\n#STEP5:\nprint ('Step5: use ARIMA. \\n Note: although we used the entire dataset for time series analysis, ideally we would perform',\n       'this analysis on just the training dataset when developing a predictive model.\\n')\nmodel = ARIMA(shampoo_series, order=(5,1,0))\nmodel_fit = model.fit(disp=0)\nprint ('fitted model summary:\\n',model_fit.summary())\n\n#Step 6: \nresiduals = DataFrame(model_fit.resid)\nprint ('Step6\" resid plot. \\n residulas describe:\\n',residuals.describe())\nresiduals.plot()\nplt.show()  \nprint ('the plot suggests there may still be some trend info not captured by the model.\\n')\nresiduals.plot(kind='kde')\nplt.show()\nprint ('we get a density plot of the residual error values, suggesting the errors are Gaussian,',\n       ' but may not be centered on zero.')\nprint ('The results show that indeed there is a bias in the prediction (a non-zero mean in the residuals).')\n\n#STEP7: Prediction\nprint('\\n\\n\\n STEP7: Prediction \\n')\nprint ('we will create train and test set and learn the model again, using Rolling Forcast\\n')\n\nX = shampoo_series.values\nsize = int(len(X) * 0.66)\ntrain, test = X[:size], X[size:]\n# OR train, test = X[0:size], X[size:len(X)]\n\nhistory = [x for x in train]\npredictions = list()\n\nfor t in range(len(test)):\n    model = ARIMA(history, order=(5,1,0))\n    model_fit = model.fit(disp=0)\n    summary = model_fit.summary()\n    \n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    \n    observation = test[t]\n    history.append(observation)\n    \n    print('predicted=%f, expected=%f' % (yhat, observation))\n    \nerror = mean_squared_error(test, predictions)\nprint ('Test MSE: %.3f' % error)\n\n#plot\nplt.plot(test)\nplt.plot(predictions, color='red')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuraing an ARIMA model\n\nThe classical approach for fitting an ARIMA model is to follow the [Box-Jenkins Methodology](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method). This is a process that uses time series analysis and diagnostics to discover good parameters for the ARIMA model. \n\nIn summary, the steps of this process are as follows:\n\n* Model Identification. Use plots and summary statistics to identify trends, seasonality, and autoregression elements to get an idea of the amount of differencing and the size of the lag that will be required.\n* Parameter Estimation. Use a fitting procedure to find the coefficients of the regression model.\n* Model Checking. Use plots and statistical tests of the residual errors to determine the amount and type of temporal structure not captured by the model.\n\nThe process is repeated until either a desirable level of fit is achieved on the in-sample or out-of-sample observations (e.g. training or test datasets).\n\nGiven that the model can be fit efficiently on modest-sized time series datasets, grid searching parameters of the model can be a valuable approach. \nFor an example of how to grid search the hyperparameters of the ARIMA model, see the tutorial: \n[How to Grid Search ARIMA Model Hyperparameters with Python](https://machinelearningmastery.com/grid-search-arima-hyperparameters-with-python/)\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom pandas import datetime\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\ndef evaluate_arima_model(X, arima_order):\n    train_size = int(len(X) * 0.66)\n    train, test = X[0:train_size], X[train_size:len(X)]\n    history = [x for x in train]\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, arima_order)\n        model_fit = model.fit(disp=0)\n        output = model_fit.forecast()\n        yhat = output[0]\n        predictions.append(yhat)\n        obs = test[t]\n        history.append(obs)\n        #print('predicted=%f, expected=%f' % (yhat, obs))\n    error = mean_squared_error(test, predictions)\n    #print('Test MSE: %.3f' % error)\n    return error, test, predictions\n\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg, best_predictions = float(\"inf\"), None, list()\n    \n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order=(p,d,q)\n                try:\n                    mse, test , predicts = evaluate_arima_model(dataset, order)\n                    if mse < best_score:\n                        best_score, best_cfg, test, best_predictions = mse, order, test, predicts\n                    print('ARIMA%s MSE=%.3f Rms=%.3f' % (order, mse, sqrt(best_score)))\n                except:\n                    continue\n    print ('Best ARIMA%s MSE=%.3f Rms=%.3f' % (best_cfg, best_score, sqrt(best_score)))\n    return (test, best_predictions)\n\n\nimport warnings\n    \ndef parser(x):\n    return datetime.strptime('190'+x, '%Y-%m')\nseries = read_csv('/kaggle/input/femalebirthday/daily-total-female-births.csv', \n                  header=0, parse_dates=[0], index_col=0, squeeze=True) \n#                  ,date_parser=parser)\n#series = shampoo_series\n\np_values = [0,1,2,4,6,8,10]\nd_values = range(0,3)\nq_values = range(0,3)\nwarnings.filterwarnings(\"ignore\")\n\ntest, best_predictions = evaluate_models(series.values, p_values, d_values, q_values)\n# plot\npyplot.plot(test)\npyplot.plot(best_predictions, color='red')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extensions\nThe grid search method used in this tutorial is simple and can easily be extended. This section lists some ideas to extend the approach you may wish to explore.\n\n* **Seed Grid**. The classical diagnostic tools of ACF and PACF plots can still be used with the results used to seed the grid of ARIMA parameters to search.\n* **Alternate Measures**. The search seeks to optimize the out-of-sample mean squared error. This could be changed to another out-of-sample statistic, an in-sample statistic, such as AIC or BIC, or some combination of the two. You can choose a metric that is most meaningful on your project.\n* **Residual Diagnostics**. Statistics can automatically be calculated on the residual forecast errors to provide an additional indication of the quality of the fit. Examples include statistical tests for whether the distribution of residuals is Gaussian and whether there is an autocorrelation in the residuals.\n* **Update Model**. The ARIMA model is created from scratch for each one-step forecast. With careful inspection of the API, it may be possible to update the internal data of the model with new observations rather than recreating it from scratch.\n* **Preconditions**. The ARIMA model can make assumptions about the time series dataset, such as normality and stationarity. These could be checked and a warning raised for a given of a dataset prior to a given model being trained.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}